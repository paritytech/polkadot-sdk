# this is an artificial job dependency, for pipeline optimization using GitLab's DAGs
# the job can be found in check.yml
.run-immediately:
  needs:
    - job: job-starter
      artifacts: false

#
#
#
.codecov-check:
  script:
    - >
      if command -v codecovcli -h >/dev/null 2>&1; then
        codecovcli --version;
      else
        echo "downloading codecovcli";
        curl -s -o codecovcli https://cli.codecov.io/latest/linux/codecov;
        chmod +x codecovcli;
        mv codecovcli /usr/local/bin/codecovcli;
      fi
    #
    - codecovcli --version

#
#
#
codecov-start:
  stage: test
  when: manual
  allow_failure: false
  extends:
    - .kubernetes-env
    - .common-refs
    - .pipeline-stopper-artifacts
    - .run-immediately
  script:
    - !reference [ .codecov-check, script ]
    - >
      if [ "$CI_COMMIT_REF_NAME" != "master" ]; then
        codecovcli -v create-commit -t ${CODECOV_TOKEN} -r paritytech/polkadot-sdk --commit-sha ${CI_COMMIT_SHA} --fail-on-error --pr ${CI_COMMIT_REF_NAME} --git-service github;
        codecovcli -v create-report -t ${CODECOV_TOKEN} -r paritytech/polkadot-sdk --commit-sha ${CI_COMMIT_SHA} --fail-on-error --pr ${CI_COMMIT_REF_NAME} --git-service github;
      else
        codecovcli -v create-commit -t ${CODECOV_TOKEN} -r paritytech/polkadot-sdk --commit-sha ${CI_COMMIT_SHA} --fail-on-error --git-service github;
        codecovcli -v create-report -t ${CODECOV_TOKEN} -r paritytech/polkadot-sdk --commit-sha ${CI_COMMIT_SHA} --fail-on-error --git-service github;      
      fi

#
#
#
codecov-finish:
  stage: test
  extends:
    - .kubernetes-env
    - .common-refs
    - .pipeline-stopper-artifacts
  needs:
    - test-linux-stable-codecov
  script:
    - !reference [ .codecov-check, script ]
    - codecovcli -v create-report-results -t ${CODECOV_TOKEN} -r paritytech/polkadot-sdk --commit-sha ${CI_COMMIT_SHA} --git-service github
    - codecovcli -v get-report-results -t ${CODECOV_TOKEN} -r paritytech/polkadot-sdk --commit-sha ${CI_COMMIT_SHA} --git-service github
    - codecovcli -v send-notifications -t ${CODECOV_TOKEN} -r paritytech/polkadot-sdk --commit-sha ${CI_COMMIT_SHA} --git-service github

#
#
#
test-linux-stable-codecov:
  stage: test
  needs:
    - codecov-start
  extends:
    - .docker-env
    - .common-refs
    - .pipeline-stopper-artifacts
  variables:
    CI_IMAGE: europe-docker.pkg.dev/parity-build/ci-images/ci-unified:bullseye-1.77.0
    RUST_TOOLCHAIN: stable
    RUSTFLAGS: "-Cdebug-assertions=y -Cinstrument-coverage"
    LLVM_PROFILE_FILE: "target/coverage/cargo-test-${CI_NODE_INDEX}-%p-%m.profraw"
    CARGO_INCREMENTAL: 0
    FORKLIFT_BYPASS: "true"
  parallel: 2
  script:
    # tools
    - !reference [ .codecov-check, script ]
    - rustup component add llvm-tools-preview
    - mkdir -p target/coverage/result/
    # Place real test call here
    - >
      time cargo nextest run -p polkadot \
        --locked \
        --release \
        --no-fail-fast \
        --partition count:${CI_NODE_INDEX}/${CI_NODE_TOTAL}
    # generate and upload reports
    - >
      grcov \
        target/coverage/ \
        --binary-path ./target/release/ \
        -s . \
        -t lcov \
        --branch \
        -o target/coverage/result/report-${CI_NODE_INDEX}.lcov
    - ls -l target/coverage/result/
    - >
      if [ "$CI_COMMIT_REF_NAME" != "master" ]; then  
        codecovcli -v do-upload -f target/coverage/result/report-${CI_NODE_INDEX}.lcov --disable-search -t ${CODECOV_TOKEN} -r paritytech/polkadot-sdk --commit-sha ${CI_COMMIT_SHA} --fail-on-error --pr ${CI_COMMIT_REF_NAME} --git-service github;
      else
        codecovcli -v do-upload -f target/coverage/result/report-${CI_NODE_INDEX}.lcov --disable-search -t ${CODECOV_TOKEN} -r paritytech/polkadot-sdk --commit-sha ${CI_COMMIT_SHA} --fail-on-error --git-service github;
      fi

test-linux-stable:
  stage: test
  extends:
    - .docker-env
    - .common-refs
    - .run-immediately
    - .pipeline-stopper-artifacts
  variables:
    RUST_TOOLCHAIN: stable
    # Enable debug assertions since we are running optimized builds for testing
    # but still want to have debug assertions.
    RUSTFLAGS: "-Cdebug-assertions=y -Dwarnings"
  parallel: 3
  script:
    # Build all but only execute 'runtime' tests.
    - echo "Node index - ${CI_NODE_INDEX}. Total amount - ${CI_NODE_TOTAL}"
    - >
      time cargo nextest run \
        --workspace \
        --locked \
        --release \
        --no-fail-fast \
        --features try-runtime,experimental,riscv,ci-only-tests \
        --partition count:${CI_NODE_INDEX}/${CI_NODE_TOTAL}
    # Upload tests results to Elasticsearch
    - echo "Upload test results to Elasticsearch"
    - cat target/nextest/default/junit.xml | xq . > target/nextest/default/junit.json
    - >
      curl -v -XPOST --http1.1 \
      -u ${ELASTIC_USERNAME}:${ELASTIC_PASSWORD} \
      https://elasticsearch.parity-build.parity.io/unit-tests/_doc/${CI_JOB_ID} \
      -H 'Content-Type: application/json' \
      -d @target/nextest/default/junit.json || echo "failed to upload junit report"
    # run runtime-api tests with `enable-staging-api` feature on the 1st node
    - if [ ${CI_NODE_INDEX} == 1 ]; then time cargo nextest run -p sp-api-test --features enable-staging-api; fi
  artifacts:
    when: always
    paths:
      - target/nextest/default/junit.xml
    reports:
      junit: target/nextest/default/junit.xml
  timeout: 90m

test-linux-oldkernel-stable:
  extends: test-linux-stable
  tags:
    - oldkernel-vm

# https://github.com/paritytech/ci_cd/issues/864
test-linux-stable-runtime-benchmarks:
  stage: test
  extends:
    - .docker-env
    - .common-refs
    - .run-immediately
    - .pipeline-stopper-artifacts
  variables:
    RUST_TOOLCHAIN: stable
    # Enable debug assertions since we are running optimized builds for testing
    # but still want to have debug assertions.
    RUSTFLAGS: "-Cdebug-assertions=y -Dwarnings"
  script:
    - time cargo nextest run --workspace --features runtime-benchmarks benchmark --locked --cargo-profile testnet

# can be used to run all tests
# test-linux-stable-all:
#   stage: test
#   extends:
#     - .docker-env
#     - .common-refs
#     - .run-immediately
#   variables:
#     RUST_TOOLCHAIN: stable
#     # Enable debug assertions since we are running optimized builds for testing
#     # but still want to have debug assertions.
#     RUSTFLAGS: "-Cdebug-assertions=y -Dwarnings"
#   parallel: 3
#   script:
#     # Build all but only execute 'runtime' tests.
#     - echo "Node index - ${CI_NODE_INDEX}. Total amount - ${CI_NODE_TOTAL}"
#     - >
#       time cargo nextest run \
#         --workspace \
#         --locked \
#         --release \
#         --no-fail-fast \
#         --features runtime-benchmarks,try-runtime \
#         --partition count:${CI_NODE_INDEX}/${CI_NODE_TOTAL}
#     # todo: add flacky-test collector

# takes about 1,5h without cache
# can be used to check that nextest works correctly
# test-linux-stable-polkadot:
#   stage: test
#   timeout: 2h
#   extends:
#     - .docker-env
#     - .common-refs
#     - .run-immediately
#     - .collect-artifacts-short
#   variables:
#     RUST_TOOLCHAIN: stable
#     # Enable debug assertions since we are running optimized builds for testing
#     # but still want to have debug assertions.
#     RUSTFLAGS: "-Cdebug-assertions=y -Dwarnings"
#   script:
#     - mkdir -p artifacts
#     - time cargo test --workspace
#       --locked
#       --profile testnet
#       --features=runtime-benchmarks,runtime-metrics,try-runtime --
#       --skip upgrade_version_checks_should_work

test-doc:
  stage: test
  extends:
    - .docker-env
    - .common-refs
  # DAG
  needs:
    - job: test-rustdoc
      artifacts: false
  variables:
    # Enable debug assertions since we are running optimized builds for testing
    # but still want to have debug assertions.
    RUSTFLAGS: "-Cdebug-assertions=y -Dwarnings"
  script:
    - time cargo test --doc --workspace

test-rustdoc:
  stage: test
  extends:
    - .docker-env
    - .common-refs
    - .run-immediately
  variables:
    SKIP_WASM_BUILD: 1
  script:
    - time cargo doc --workspace --all-features --no-deps

quick-benchmarks-omni:
  stage: test
  extends:
    - .docker-env
    - .common-refs
    - .run-immediately
  variables:
    # Enable debug assertions since we are running optimized builds for testing
    # but still want to have debug assertions.
    RUSTFLAGS: "-C debug-assertions"
    RUST_BACKTRACE: "full"
    WASM_BUILD_NO_COLOR: 1
    WASM_BUILD_RUSTFLAGS: "-C debug-assertions"
  script:
    - time cargo build --locked --quiet --release -p asset-hub-westend-runtime --features runtime-benchmarks
    - time cargo run --locked --release -p frame-omni-bencher --quiet -- v1 benchmark pallet --runtime target/release/wbuild/asset-hub-westend-runtime/asset_hub_westend_runtime.compact.compressed.wasm --all --steps 2 --repeat 1 --quiet

test-linux-stable-int:
  stage: test
  extends:
    - .docker-env
    - .common-refs
    - .run-immediately
    - .pipeline-stopper-artifacts
  variables:
    # Enable debug assertions since we are running optimized builds for testing
    # but still want to have debug assertions.
    RUSTFLAGS: "-C debug-assertions -D warnings"
    RUST_BACKTRACE: 1
    WASM_BUILD_NO_COLOR: 1
    WASM_BUILD_RUSTFLAGS: "-C debug-assertions -D warnings"
    # Ensure we run the UI tests.
    RUN_UI_TESTS: 1
  script:
    - WASM_BUILD_NO_COLOR=1
      time cargo test -p staging-node-cli --release --locked -- --ignored
